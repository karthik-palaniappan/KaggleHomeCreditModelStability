{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":680.74407,"end_time":"2024-04-13T08:13:15.231681","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-13T08:01:54.487611","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n\nfrom sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import KNNImputer","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":6.374269,"end_time":"2024-04-13T08:02:03.574905","exception":false,"start_time":"2024-04-13T08:01:57.200636","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-11T08:31:42.695826Z","iopub.execute_input":"2024-05-11T08:31:42.696237Z","iopub.status.idle":"2024-05-11T08:31:51.204715Z","shell.execute_reply.started":"2024-05-11T08:31:42.696204Z","shell.execute_reply":"2024-05-11T08:31:51.203807Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Pipeline:\n\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n        \n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df\n\n\nclass Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        \n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return expr_max +expr_last+expr_mean\n    \n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return  expr_max +expr_last+expr_mean\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return  expr_max +expr_last#+expr_count\n    \n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs\n\ndef read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1,2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        chunks.append(df)\n    \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df\n\ndef feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base\n\ndef to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    return df_data, cat_cols\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"papermill":{"duration":0.049213,"end_time":"2024-04-13T08:02:03.63016","exception":false,"start_time":"2024-04-13T08:02:03.580947","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T14:58:00.300691Z","iopub.execute_input":"2024-05-08T14:58:00.301387Z","iopub.status.idle":"2024-05-08T14:58:00.339833Z","shell.execute_reply.started":"2024-05-08T14:58:00.301335Z","shell.execute_reply":"2024-05-08T14:58:00.338758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"","metadata":{"papermill":{"duration":0.012664,"end_time":"2024-04-13T08:02:03.648466","exception":false,"start_time":"2024-04-13T08:02:03.635802","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T14:58:00.341059Z","iopub.execute_input":"2024-05-08T14:58:00.341393Z","iopub.status.idle":"2024-05-08T14:58:00.364419Z","shell.execute_reply.started":"2024-05-08T14:58:00.341346Z","shell.execute_reply":"2024-05-08T14:58:00.363564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n    ]\n}","metadata":{"papermill":{"duration":145.109908,"end_time":"2024-04-13T08:04:28.763968","exception":false,"start_time":"2024-04-13T08:02:03.65406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T14:58:00.366854Z","iopub.execute_input":"2024-05-08T14:58:00.367683Z","iopub.status.idle":"2024-05-08T15:00:17.207599Z","shell.execute_reply.started":"2024-05-08T14:58:00.367645Z","shell.execute_reply":"2024-05-08T15:00:17.206494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_train = feature_eng(**data_store)\ndel data_store\ngc.collect()\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)\nnums=df_train.select_dtypes(exclude='category').columns\nfrom itertools import combinations, permutations\nnans_df = df_train[nums].isna()\nnans_groups={}\nfor col in nums:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\ndef reduce_group(grps):\n    use = []\n    for g in grps:\n        mx = 0; vx = g[0]\n        for gg in g:\n            n = df_train[gg].nunique()\n            if n>mx:\n                mx = n\n                vx = gg\n        use.append(vx)\n    return use\n\ndef group_columns_by_correlation(matrix, threshold=0.8):\n    correlation_matrix = matrix.corr()\n    groups = []\n    remaining_cols = list(matrix.columns)\n    while remaining_cols:\n        col = remaining_cols.pop(0)\n        group = [col]\n        correlated_cols = [col]\n        for c in remaining_cols:\n            if correlation_matrix.loc[col, c] >= threshold:\n                group.append(c)\n                correlated_cols.append(c)\n        groups.append(group)\n        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n    \n    return groups\n\nuses=[]\nfor k,v in nans_groups.items():\n    if len(v)>1:\n            Vs = nans_groups[k]\n            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n            use=reduce_group(grps)\n            uses=uses+use\n    else:\n        uses=uses+v\nprint(uses)\nprint(len(uses))\nuses=uses+list(df_train.select_dtypes(include='category').columns)\nprint(len(uses))\ndf_train=df_train[uses]","metadata":{"papermill":{"duration":106.862548,"end_time":"2024-04-13T08:06:15.632273","exception":false,"start_time":"2024-04-13T08:04:28.769725","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:00:17.209040Z","iopub.execute_input":"2024-05-08T15:00:17.209840Z","iopub.status.idle":"2024-05-08T15:01:55.779375Z","shell.execute_reply.started":"2024-05-08T15:00:17.209800Z","shell.execute_reply":"2024-05-08T15:01:55.778412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\ndevice='gpu'\n#n_samples=200000\nn_est=6000\nDRY_RUN = True if sample.shape[0] == 10 else False   \nif DRY_RUN:\n    device='cpu'\n    df_train = df_train.iloc[:50000]\n    #n_samples=10000\n    n_est=600\nprint(device)","metadata":{"papermill":{"duration":0.026382,"end_time":"2024-04-13T08:06:15.668182","exception":false,"start_time":"2024-04-13T08:06:15.6418","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:01:55.780528Z","iopub.execute_input":"2024-05-08T15:01:55.780823Z","iopub.status.idle":"2024-05-08T15:01:55.800401Z","shell.execute_reply.started":"2024-05-08T15:01:55.780798Z","shell.execute_reply":"2024-05-08T15:01:55.799417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n    ]\n}","metadata":{"papermill":{"duration":0.36519,"end_time":"2024-04-13T08:06:16.044056","exception":false,"start_time":"2024-04-13T08:06:15.678866","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:01:55.801886Z","iopub.execute_input":"2024-05-08T15:01:55.802497Z","iopub.status.idle":"2024-05-08T15:01:56.244346Z","shell.execute_reply.started":"2024-05-08T15:01:55.802462Z","shell.execute_reply":"2024-05-08T15:01:56.243414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = feature_eng(**data_store)\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)\n\ndf_test, cat_cols = to_pandas(df_test, cat_cols)\ndf_test = reduce_mem_usage(df_test)\nweek_num = list(df_test[\"WEEK_NUM\"])\ngc.collect()","metadata":{"papermill":{"duration":0.590897,"end_time":"2024-04-13T08:06:16.644692","exception":false,"start_time":"2024-04-13T08:06:16.053795","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:01:56.245530Z","iopub.execute_input":"2024-05-08T15:01:56.245883Z","iopub.status.idle":"2024-05-08T15:01:56.770899Z","shell.execute_reply.started":"2024-05-08T15:01:56.245858Z","shell.execute_reply":"2024-05-08T15:01:56.769828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection","metadata":{"papermill":{"duration":0.009514,"end_time":"2024-04-13T08:06:16.664066","exception":false,"start_time":"2024-04-13T08:06:16.654552","status":"completed"},"tags":[]}},{"cell_type":"code","source":"y = df_train[\"target\"]\nweeks = df_train[\"WEEK_NUM\"]\ndf_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\ncv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n","metadata":{"papermill":{"duration":0.150113,"end_time":"2024-04-13T08:06:16.823803","exception":false,"start_time":"2024-04-13T08:06:16.67369","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:01:56.772318Z","iopub.execute_input":"2024-05-08T15:01:56.772742Z","iopub.status.idle":"2024-05-08T15:01:56.944754Z","shell.execute_reply.started":"2024-05-08T15:01:56.772708Z","shell.execute_reply":"2024-05-08T15:01:56.943764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[cat_cols] = df_train[cat_cols].astype(str)\ndf_test[cat_cols] = df_test[cat_cols].astype(str)","metadata":{"papermill":{"duration":0.32656,"end_time":"2024-04-13T08:06:17.160488","exception":false,"start_time":"2024-04-13T08:06:16.833928","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:01:56.946389Z","iopub.execute_input":"2024-05-08T15:01:56.947102Z","iopub.status.idle":"2024-05-08T15:01:57.261110Z","shell.execute_reply.started":"2024-05-08T15:01:56.947064Z","shell.execute_reply":"2024-05-08T15:01:57.260029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2000,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": 42,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": device, \n    \"verbose\": -1,\n}","metadata":{"papermill":{"duration":0.018777,"end_time":"2024-04-13T08:06:17.190852","exception":false,"start_time":"2024-04-13T08:06:17.172075","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:01:57.264557Z","iopub.execute_input":"2024-05-08T15:01:57.264844Z","iopub.status.idle":"2024-05-08T15:01:57.270566Z","shell.execute_reply.started":"2024-05-08T15:01:57.264819Z","shell.execute_reply":"2024-05-08T15:01:57.269666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_2 = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 8,  \n    \"learning_rate\": 0.03,\n    \"n_estimators\": 2000,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": 42,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':50,\n    \"device\": device, \n    \"verbose\": -1,\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:01:57.271678Z","iopub.execute_input":"2024-05-08T15:01:57.271922Z","iopub.status.idle":"2024-05-08T15:01:57.282440Z","shell.execute_reply.started":"2024-05-08T15:01:57.271900Z","shell.execute_reply":"2024-05-08T15:01:57.281632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom catboost import CatBoostClassifier, Pool\n\nfitted_models_cat = []\nfitted_models_lgb = []\n\ncv_scores_cat = []\ncv_scores_lgb = []\n\niterator = 0\nfor idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n    train_pool = Pool(X_train, y_train,cat_features=cat_cols)\n    val_pool = Pool(X_valid, y_valid,cat_features=cat_cols)\n    clf = CatBoostClassifier(\n    eval_metric='AUC',\n    task_type='GPU',\n    learning_rate=0.03,\n    iterations=n_est)\n    random_seed=3107\n    clf.fit(train_pool, eval_set=val_pool,verbose=300)\n    fitted_models_cat.append(clf)\n    y_pred_valid = clf.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_cat.append(auc_score)\n    \n    \n    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n    \n    if iterator % 2 == 0:\n        model = lgb.LGBMClassifier(**params)\n    else:\n        model = lgb.LGBMClassifier(**params_2)\n    model.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n    \n    fitted_models_lgb.append(model)\n    y_pred_valid = model.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_lgb.append(auc_score)\n    \n    iterator += 1\n    \n    \nprint(\"CV AUC scores: \", cv_scores_cat)\nprint(\"Maximum CV AUC score: \", max(cv_scores_cat))\n\n\nprint(\"CV AUC scores: \", cv_scores_lgb)\nprint(\"Maximum CV AUC score: \", max(cv_scores_lgb))","metadata":{"papermill":{"duration":416.124983,"end_time":"2024-04-13T08:13:13.325841","exception":false,"start_time":"2024-04-13T08:06:17.200858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:01:57.283448Z","iopub.execute_input":"2024-05-08T15:01:57.284051Z","iopub.status.idle":"2024-05-08T15:08:43.638776Z","shell.execute_reply.started":"2024-05-08T15:01:57.284028Z","shell.execute_reply":"2024-05-08T15:08:43.637629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        \n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n        \n        X[cat_cols] = X[cat_cols].astype(\"category\")\n        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[5:]]\n        \n        return np.mean(y_preds, axis=0)\n\nmodel = VotingModel(fitted_models_cat+fitted_models_lgb)","metadata":{"papermill":{"duration":0.024494,"end_time":"2024-04-13T08:13:13.36426","exception":false,"start_time":"2024-04-13T08:13:13.339766","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:08:43.640640Z","iopub.execute_input":"2024-05-08T15:08:43.641459Z","iopub.status.idle":"2024-05-08T15:08:43.650580Z","shell.execute_reply.started":"2024-05-08T15:08:43.641420Z","shell.execute_reply":"2024-05-08T15:08:43.649590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submision","metadata":{"papermill":{"duration":0.012679,"end_time":"2024-04-13T08:13:13.389984","exception":false,"start_time":"2024-04-13T08:13:13.377305","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_test = df_test.drop(columns=[\"WEEK_NUM\"])\ndf_test = df_test.set_index(\"case_id\")\n\n\ny_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\nsubm_df = pd.read_csv(ROOT / \"sample_submission.csv\")\nsubm_df = subm_df.set_index(\"case_id\")\nsubm_df[\"score\"] = y_pred\nsubm_df[\"WEEK_NUM\"] = week_num\ndisplay(subm_df.head())\nprint(\"Check null: \", subm_df[\"score\"].isnull().any())\ncondition = subm_df[\"WEEK_NUM\"] < (subm_df[\"WEEK_NUM\"].max() - subm_df[\"WEEK_NUM\"].min())/2 + subm_df[\"WEEK_NUM\"].min() \nsubm_df.loc[condition, 'score'] = (subm_df.loc[condition, 'score'] - 0.02).clip(0) \nsubm_df","metadata":{"papermill":{"duration":0.749525,"end_time":"2024-04-13T08:13:14.152046","exception":false,"start_time":"2024-04-13T08:13:13.402521","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:08:43.651713Z","iopub.execute_input":"2024-05-08T15:08:43.651988Z","iopub.status.idle":"2024-05-08T15:08:44.375055Z","shell.execute_reply.started":"2024-05-08T15:08:43.651963Z","shell.execute_reply":"2024-05-08T15:08:44.373971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del subm_df[\"WEEK_NUM\"]\nsubm_df.to_csv(\"submission.csv\")\nsubm_df","metadata":{"papermill":{"duration":0.029131,"end_time":"2024-04-13T08:13:14.196222","exception":false,"start_time":"2024-04-13T08:13:14.167091","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-08T15:08:44.376521Z","iopub.execute_input":"2024-05-08T15:08:44.377586Z","iopub.status.idle":"2024-05-08T15:08:44.391964Z","shell.execute_reply.started":"2024-05-08T15:08:44.377547Z","shell.execute_reply":"2024-05-08T15:08:44.391027Z"},"trusted":true},"execution_count":null,"outputs":[]}]}